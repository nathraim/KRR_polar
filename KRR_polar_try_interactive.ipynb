{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt at making an interactive version of KRR\n",
    "## With ipywidget\n",
    "\n",
    "A KRR prediction of each individual polarizability component $\\alpha_{\\gamma \\delta}$ reads\n",
    "$$\\alpha^\\text{KRR}_{\\gamma \\delta}(\\mathcal A)=\\bar{\\alpha}^\\text{ai}_{\\gamma \\delta}+\\sum_{j=1}^{N} w_{j}^{\\gamma\\delta} k(\\mathcal A,\\mathcal A_j) $$\n",
    "The kernel entering KRR is based on a Gaussian similarity measure between structures $\\mathcal A$, given by\n",
    "$$k(\\mathcal A,\\mathcal A_j)= e^{-\\frac{|\\boldsymbol{u}(\\mathcal A)-\\boldsymbol{u}(\\mathcal A_j)|^2}{2\\sigma^2}}$$\n",
    "with $\\sigma$ being  a  hyperparameter  that  controls  the magnitude of the correlation between training points.\n",
    "The value of $\\sigma$ which needs to be tuned by cross-validation to maximize the prediction accuracy. $\\boldsymbol{u}$ is a vector that has the role of mapping the atomic coordinates of the structure $\\mathcal A$ to a given representation of dimension $M$.\n",
    "The regression weights $\\boldsymbol{w}^{\\gamma\\delta}$ are obtained by minimizing a loss function regularized by an $L2$-norm [1] over the training set. This procedure leads to the following expression\n",
    "\\begin{equation}\n",
    " \\boldsymbol{w}^{\\gamma\\delta}=\\left(\\mathbb{K}+\\eta \\mathbb{1} \\right)^{-1}\\cdot \\Delta\\boldsymbol{\\alpha}_{\\gamma \\delta} \\,,\n",
    "\\end{equation}\n",
    "where $\\mathbb{1}$ is the identity matrix, $\\mathbb{K}$ is the $N \\times N$ kernel matrix associated with the reference structures, such that $\\mathbb{K}_{ij}=k(\\mathcal A_i,\\mathcal A_j), i,j=1,... ,N$, and $\\eta$ is the regularization parameter which controls to which extent the fitted data can deviate from the training points.\n",
    "The quantity $\\Delta\\boldsymbol{\\alpha}_{\\gamma \\delta}$ represents the vector containing all $N$ entries in the training set of $\\Delta \\alpha_{\\gamma \\delta}^j=\\alpha^\\text{ai}_{\\gamma \\delta}(\\mathcal A_j)-\\bar{\\alpha}^\\text{ai}_{\\gamma \\delta}$.\n",
    "\n",
    "The efficiency of any GPR model strongly depends on the quality of the representation(=descriptor) that is encoded in $\\boldsymbol{u}(\\mathcal A)$. For a representation to be efficient, it should contain the least possible number of elements to express the unicity of a given structure and avoid redundant information.\n",
    "\n",
    "Here we shall choose very simply $\\boldsymbol{u}(\\mathcal A)$ to contain all atomic positions, i.e., $\\boldsymbol{u}(\\mathcal A)=\\{r_i\\},i=1,...,N_{\\text{atoms}}$.\n",
    "The system we are going to have a look at is the molecule of paracetamol (C$_{8}$H$_9$O$_2$N), shown below:\n",
    "<img src=\"para_mol.png\" width=\"200\">\n",
    "\n",
    "In this notebook, we simply show how the choice of the hyperparameters and number of training points affects the quality of the prediction of polarizability tensors (and Raman spectra).\n",
    "\n",
    "With a grid search, one can then draw a profile for the error, e.g., in the form of a contour plot like this:\n",
    "<img src=\"contour_hyperparameters.png\" width=\"400\">\n",
    "Here yellow means a high error, while blue means a low error.\n",
    "\n",
    "# Tasks\n",
    "- <span style=\"color: #cc0000\">Move the sliders below and look at the results.</span>\n",
    "- <span style=\"color: #cc0000\">Gauge the impact of the hyperparameters.</span>\n",
    "- <span style=\"color: #cc0000\">See how the machine-learned polarizability tensors match the _ab initio_ calculations when increasing the number of training points (the size of the test set is here fixed).</span>\n",
    "- <span style=\"color: #cc0000\">Try different descriptors (not yet available).</span>\n",
    "\n",
    "# Visualization\n",
    "With the following selectors, you can pick:\n",
    "- the number of training points $N_{\\text{training}}$\n",
    "- $\\sigma$, a hyperparameter known as length scale\n",
    "- $\\lambda$, a regularization hyperparameter\n",
    "\n",
    "Simply execute the whole notebook and play with the sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bqplot.pyplot as pl\n",
    "plot_time_series = pl.figure()\n",
    "plot_raman_spec = pl.figure()\n",
    "#pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.special import erf\n",
    "import scipy.spatial\n",
    "import os,argparse\n",
    "import random\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def file_len(fname):\n",
    "    '''\n",
    "    This function gives the number of lines of a given text file\n",
    "    '''\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "def read_txt(name,array):\n",
    "    '''\n",
    "    The following function reads a text file containing alternatively one line of comment and one line of data, and returns a numpy array\n",
    "    '''\n",
    "    name = os.path.join('data', name)\n",
    "    N_points = int(file_len(name)/2)\n",
    "    #print ('There are', N_points, 'points in the file', name)\n",
    "    data = open(name,'r')\n",
    "    array = []\n",
    "    counter = 0\n",
    "    while (counter < N_points):\n",
    "        line = data.readline() # Skip line of comment\n",
    "        if not line:\n",
    "            break\n",
    "        line = data.readline() # Line containing the information wanted\n",
    "        array.append(list(map(float, line.split()[0:])))\n",
    "        counter = counter+1\n",
    "    data.close()\n",
    "    array = np.array(array)\n",
    "    return array\n",
    "\n",
    "# The following function reads a numpy file (.npy) containing the quantity of interest stored directly as an array.\n",
    "\n",
    "def read_npy(name,array):\n",
    "    '''\n",
    "    The following function reads a numpy array.\n",
    "    '''\n",
    "    name = os.path.join('data', name)\n",
    "#   Load the saved numpy array\n",
    "    array_reloaded = np.load(name)\n",
    "#   Determine length of the array (corresponds to the number of training (or test) points)\n",
    "    N_points = len(array_reloaded)\n",
    "    #print ('There are', N_points, 'points in the file', name)\n",
    "    return array_reloaded\n",
    "\n",
    "def read_data(name,array):\n",
    "    '''\n",
    "    Reads data either as text or numpy format\n",
    "    '''\n",
    "    extension = os.path.splitext(name)[1]\n",
    "    if (extension == '.dat'):\n",
    "        array = read_txt(name,array)\n",
    "    elif (extension == '.npy'):\n",
    "        array = read_npy(name,array)\n",
    "    else:\n",
    "        print ('Extension for %s unrecognized !'.format(name))\n",
    "        exit()\n",
    "    return array\n",
    "\n",
    "\n",
    "def shuffle_data(array,index_shuf):\n",
    "    '''\n",
    "    Shuffle an array according to the list of indexes given in input\n",
    "    '''\n",
    "    array_shuf = []\n",
    "    for i in index_shuf:\n",
    "        array_shuf.append(array[i])\n",
    "    return array_shuf\n",
    "\n",
    "def select_data(array,N):\n",
    "    '''\n",
    "    Selects only the first N values of the input array\n",
    "    '''\n",
    "    array = array[0:N]\n",
    "    array = np.array(array)\n",
    "    return array\n",
    "\n",
    "# def print_error(sigma,polar_dfpt, polar_ML,f_,f_polar,f_Raman_predict,f_Raman_dfpt,N_training):\n",
    "def print_error(sigma,polar_dfpt,polar_ML,f_Raman_predict,f_Raman_dfpt,N_training):\n",
    "    '''\n",
    "    Print error on screen and write it to file\n",
    "    '''\n",
    "    mae = np.mean(abs(polar_dfpt - polar_ML),axis=0) # MAE\n",
    "    max_error = np.amax(abs(polar_dfpt - polar_ML),axis=0)\n",
    "    min_error = np.amin(abs(polar_dfpt - polar_ML),axis=0)\n",
    "    rmse = np.sqrt(np.mean((polar_dfpt - polar_ML)**2,axis=0)) # RMSE\n",
    "    std = np.std(polar_dfpt,axis=0) # STD\n",
    "    rmse_normal = 100*rmse/std\n",
    "#     print (\"Error                     |      xx          yy          zz          xy          xz          yz\")\n",
    "#     print (\"--------------------------|---------------------------------------------------------------------\")\n",
    "#     print (\"MAE                       |\", end=' ')\n",
    "#     for a in mae: print (\"{0:.4e}\".format(a),end=' ')\n",
    "#     print ('')\n",
    "#     print (\"Max                       |\",end=' ')\n",
    "#     for a in max_error: print (\"{0:.4e}\".format(a),end=' ')\n",
    "#     print ('')\n",
    "#     print (\"Min                       |\",end=' ')\n",
    "#     for a in min_error: print (\"{0:.4e}\".format(a),end=' ')\n",
    "#     print ('')\n",
    "#     print (\"RMSE/STD                  |\", end= ' ')\n",
    "#     for a in rmse_normal: print (\"{0:.4e}\".format(a), end='  ')\n",
    "#     print ('')\n",
    "\n",
    "    # Write to file the error versus sigma and Lambda\n",
    "    #f_.write( (\"{0:.2e}\".format(sigma)) + \" \" + \"{0:.2e}\".format(Lambda) + \" \" +(\"{:d}\".format(N_training)) + \" \" )\n",
    "    #for a in mae: f_.write( (\"{0:.5e}\".format(a)) + \" \" )\n",
    "    #for a in rmse_normal: f_.write( (\"{0:.5e}\".format(a)) + \" \" )\n",
    "    #f_.write(\"\\n\")\n",
    "\n",
    "    #f_polar.write(\"# num | polar_DFPTxx | predictionxx | polar_DFPTyy | predictionyy | ... \")\n",
    "    #for a in rmse_normal: f_polar.write( (\"{0:.5e}\".format(a)) + \" \" )\n",
    "    #f_polar.write(\"\\n\")\n",
    "    #for i in range(len(polar_dfpt)):\n",
    "    #    f_polar.write(str(i)+\" \")\n",
    "    #    for comp in range(6):\n",
    "    #        f_polar.write(str(polar_dfpt[i,comp]) + \" \" + str(polar_ML[i,comp]) + \" \")\n",
    "    #    f_polar.write(\"\\n\")\n",
    "\n",
    "    for i in range(len(polar_dfpt)):\n",
    "        f_Raman_predict.write(str(polar_ML[i,0]) + \" \" + str(polar_ML[i,1]) + \" \" + str(polar_ML[i,2]) + \" \" + str(polar_ML[i,3]) + \" \" + str(polar_ML[i,4]) + \" \" + str(polar_ML[i,5]) + '\\n' )\n",
    "\n",
    "    # Write the corresponding true spectrum (i.e. DFPT on training set) \n",
    "    for i in range(len(polar_dfpt)):\n",
    "        f_Raman_dfpt.write(str(polar_dfpt[i,0]) + \" \" + str(polar_dfpt[i,1]) + \" \" + str(polar_dfpt[i,2]) + \" \"  + str(polar_dfpt[i,3]) + \" \" + str(polar_dfpt[i,4]) + \" \" + str(polar_dfpt[i,5]) + '\\n' )\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "def compute_Raman(name_raman_dfpt,name_raman_ML):\n",
    "    '''\n",
    "    Computes polarizability autocorrelation function and Raman spectrum\n",
    "    '''\n",
    "    name_list = \"list.dat\" # Input file for autocorrelation script\n",
    "    os.chdir(\"predictions\")\n",
    "    \n",
    "#     with open(name_list,'w') as f_:\n",
    "#         f_.write(name_raman_dfpt + ' ') # The space is needed so that the autocorrelation script parses the name correctly\n",
    "#     # Calculate autocorrelation and Raman spectrum of dfpt\n",
    "#     os.system(\"python3 autocorr.py control.autocorr.in\")\n",
    "    \n",
    "    with open(name_list,'w') as f_:\n",
    "        f_.write(name_raman_ML + ' ')\n",
    "    # Calculate autocorrelation and Raman spectrum of prediction\n",
    "    os.system(\"python3 autocorr.py control.autocorr.in\")\n",
    "    \n",
    "    # Move Raman data to the \"plots\" directory\n",
    "    os.chdir(\"..\")\n",
    "#     os.rename('predictions/Raman_'+name_raman_dfpt, 'plots/Raman_'+name_raman_dfpt)\n",
    "    os.rename('predictions/Raman_'+name_raman_ML, 'plots/Raman_'+name_raman_ML)    \n",
    "  \n",
    "    freqs, intensity = np.genfromtxt('plots/Raman_'+name_raman_dfpt, usecols = (0, 1), unpack=True)   \n",
    "    freqs_krr, intensity_krr = np.genfromtxt('plots/Raman_'+name_raman_ML, usecols = (0, 1), unpack=True)    \n",
    "    \n",
    "    return intensity,intensity_krr\n",
    "    \n",
    "def plot_raman(file_path, file_name, file_name2,xmin,xmax,ymax):\n",
    "\n",
    "##################### Read Data #####################\n",
    "\n",
    "    results_file_dir = os.path.join(file_path, file_name)\n",
    "    results_file = open(results_file_dir,'r')\n",
    "    my_file = 'plot_raman_krrvsdfpt.png'    \n",
    "        \n",
    "    freqs = []\n",
    "    intensity = []\n",
    "      \n",
    "    for line in results_file:\n",
    "        \n",
    "        freqs.append(line.split()[0])\n",
    "        intensity.append(line.split()[1])\n",
    "    \n",
    "    results_file.close()\n",
    "    \n",
    "    results_file_dir = os.path.join(file_path, file_name2)\n",
    "    results_file = open(results_file_dir,'r')\n",
    "    \n",
    "    freqs_krr = []\n",
    "    intensity_krr = []\n",
    "    \n",
    "    for line in results_file:\n",
    "        \n",
    "        freqs_krr.append(line.split()[0])\n",
    "        intensity_krr.append(line.split()[1])\n",
    "        \n",
    "    results_file.close()\n",
    "    #print(freqs)\n",
    "            \n",
    "      ##################### Plotting #####################\n",
    "    \n",
    "    pl.figure(fig=plot_raman_spec)    \n",
    "    pl.clear()\n",
    "    #plt.figure(figsize=(12,6))\n",
    "    \n",
    "#     x = [float(i) for i in freqs]\n",
    "#     y = [float(i) for i in intensity]        \n",
    "#     pl.plot(x, y, '-', lw=2.5, color = 'black')\n",
    "#     x = [float(i) for i in freqs_krr]\n",
    "#     y = [float(i) for i in intensity_krr] \n",
    "#     pl.plot(x, y, '-', lw=2.5, color = 'orange')\n",
    "\n",
    "#     #plt.plot(np.array(freqs), np.array(intensity), '-', lw=2.5, color = 'black')\n",
    "#     #plt.plot(freqs_krr, intensity_krr, '-', lw=2.5, color = 'orange')\n",
    "#     pl.plot(markersize=5,markeredgewidth=5)\n",
    "            \n",
    "#     components = ['DFPT','GPR']\n",
    "\n",
    "#     pl.legend(components, bbox_to_anchor=(0.55, 0.9), loc='upper left', fontsize=24, frameon=False)\n",
    "    \n",
    "#     pl.xticks(fontsize=24)\n",
    "#     pl.yticks(fontsize=24)\n",
    "#     pl.xlim(xmin,xmax)\n",
    "#     pl.ylim(0,ymax)\n",
    "#     pl.xlabel('Wavenumber (cm$^{-1}$)', fontsize=28)\n",
    "#     pl.ylabel('$I(\\\\omega)$', fontsize=28)\n",
    "#     pl.gca().axes.get_yaxis().set_ticks([]) # Removes ticks for y axis     \n",
    "    \n",
    "#     pl.show()\n",
    "    \n",
    "    \n",
    "    global raman0\n",
    "    global raman1\n",
    "    x = [float(i) for i in freqs]\n",
    "    y = [float(i) for i in intensity] \n",
    "    raman0=pl.plot(x,y, colors = [\"black\"],labels=['DFPT'])\n",
    "    x = [float(i) for i in freqs_krr]\n",
    "    y = [float(i) for i in intensity_krr]\n",
    "    raman1=pl.plot(x,y, colors = [\"orange\"], labels=['KRR'])\n",
    "    \n",
    "    pl.xlim(xmin,xmax)\n",
    "    pl.ylim(0,ymax)\n",
    "    pl.xlabel('Wavenumber (cm$^{-1}$)', fontsize=32)\n",
    "    pl.ylabel('$I(\\\\omega)$', fontsize=32)\n",
    "    pl.legend()\n",
    "    pl.title('Machine-learned vibrational Raman spectrum')\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "\n",
    "# def plot_polar_series(polar_dfpt,polar_krr,xmin,xmax,ymin,ymax):\n",
    "#     import matplotlib.font_manager as font_manager\n",
    "            \n",
    "#       ##################### Plotting #####################\n",
    "        \n",
    "#     number = [i for i in range(len(polar_dfpt))]\n",
    "    \n",
    "#     plt.figure(figsize=(12,6))\n",
    "\n",
    "#     plt.plot(number, polar_dfpt[:,0], '-', color = 'black',lw=2)\n",
    "#     plt.plot(number, polar_krr[:,0], '-', color = 'orange',lw=2)\n",
    "\n",
    "#     #plt.plot(number, zz_actual, '-', color = 'c')\n",
    "#     #plt.plot(number, zz_predict, '-', color = 'y')\n",
    "\n",
    "#     components = ['DFPT','GPR']\n",
    "\n",
    "#     plt.legend(components, bbox_to_anchor=(0.10, 0.97), loc='upper left', fontsize=24,frameon=False,prop=font_manager.FontProperties(weight='normal',style='normal', size=26))\n",
    "\n",
    "#     plt.xlim(xmin,xmax)\n",
    "#     plt.ylim(ymin,ymax)\n",
    "#     plt.xticks(fontsize=28)\n",
    "#     plt.yticks(fontsize=28)\n",
    "#     plt.xlabel('time step', fontsize=32)\n",
    "#     plt.ylabel('$\\\\alpha_{xx}$', fontsize=32)\n",
    "#     #plt.savefig('plot_polar_series_xx', bbox_inches ='tight', dpi=300)\n",
    "#     plt.show()\n",
    "#     return plt\n",
    "\n",
    "def plot_polar_series(polar_dfpt,polar_krr,xmin,xmax,ymin,ymax):\n",
    "    import matplotlib.font_manager as font_manager\n",
    "            \n",
    "      ##################### Plotting #####################\n",
    "        \n",
    "    number = [i for i in range(len(polar_dfpt))]\n",
    "    \n",
    "    pl.figure(fig=plot_time_series)    \n",
    "    pl.clear()\n",
    "    #plt.figure(figsize=(12,6))\n",
    "\n",
    "    global seg_pl0\n",
    "    global seg_pl1\n",
    "    seg_pl0=pl.plot(number,polar_dfpt[:,0], colors = [\"black\"],labels=['DFPT'])\n",
    "    seg_pl1=pl.plot(number,polar_krr[:,0], colors = [\"orange\"], labels=['KRR'])\n",
    "    #seg_pl0.x=number\n",
    "    #seg_pl0.y=polar_dfpt[:,0]\n",
    "    #seg_pl1.x=number\n",
    "    #seg_pl1.y=polar_krr[:,0]\n",
    "    \n",
    "    #pl.plot(number, polar_dfpt[:,0], '-', color = 'black',lw=2)\n",
    "    #pl.plot(number, polar_krr[:,0], '-', color = 'orange',lw=2)    \n",
    "\n",
    "    #pl.legend(components, bbox_to_anchor=(0.10, 0.97), loc='upper left', fontsize=24,frameon=False,prop=font_manager.FontProperties(weight='normal',style='normal', size=26))\n",
    "\n",
    "    pl.xlim(xmin,xmax)\n",
    "    pl.ylim(ymin,ymax)\n",
    "    #pl.xticks(fontsize=28)\n",
    "    #pl.yticks(fontsize=28)\n",
    "    pl.xlabel('time step', fontsize=32)\n",
    "    pl.ylabel('$\\\\alpha_{xx}$', fontsize=32)\n",
    "    pl.legend()\n",
    "    pl.title('Learning polarizability tensors')\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "\n",
    "# Define maximum and minimum values for hyperparameters in case a grid search is desired\n",
    "Lambda_min = 1.e-8\n",
    "Lambda_max = 1\n",
    "sigma_min = 1.e-2\n",
    "sigma_max = 100\n",
    "#sigma = sigma_min # Current sigma\n",
    "Lambda = Lambda_min # current Lambda\n",
    "#sigma_opt = sigma # Optimal sigma\n",
    "Lambda_opt = Lambda # Optimal lambda\n",
    "\n",
    "def vary_hyper(sigma,Lambda,sigma_min,sigma_max,Lambda_min,Lambda_max):\n",
    "    '''\n",
    "    Vary hyperparameters when a grid search is requested\n",
    "    '''\n",
    "    if (Lambda<Lambda_max):\n",
    "        Lambda=Lambda*10**1\n",
    "        repeat = True\n",
    "    elif (sigma<sigma_max):\n",
    "        #sigma = sigma*10**0.25\n",
    "        sigma = sigma*10**0.5\n",
    "        Lambda = Lambda_min\n",
    "        repeat = True\n",
    "    else:\n",
    "        repeat = False\n",
    "    return sigma,Lambda,repeat\n",
    "  \n",
    "\n",
    "def krr(N_training,sigma,Lambda):\n",
    "#if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description = ''' This program will interpolate polarizability tensors based on their geometrical features (atomic coordinates, atomic density, etc.) \n",
    "# Al  l specifications are given in the \\'control_KRR\\' file.\n",
    "#     It requires at least 2 files for the training set:\n",
    "#      - 1 file containing the geometrical information of your system for each structure (i.e., atomic coordinates, atomic densities, etc.)\n",
    "#      - 1 file containing the polarizabilities (or dipoles, etc.) corresponding to the aforementioned structures \n",
    "#     and 1 file for the test set:\n",
    "#      - 1 file containing the geometrical information of your system for each structure, different from the training set\n",
    "#     After training, the routine will predict the polarizability tensors of the new (test) set of structures.\n",
    "#     The script accepts data as text (\\'.dat\\' files) or numpy format (\\'.npy\\' files). In the case of text data, it is expected that there is 1 alternatively 1 line of comment, and 1 line of data, with the elements to be predicted being separated by spaces\n",
    "#     ''',formatter_class=argparse.RawTextHelpFormatter)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "\n",
    "    # I) Preparation\n",
    "    \n",
    "    baselining = False\n",
    "    #N_training = sys.argv[1]\n",
    "\n",
    "    # a) Read control file\n",
    "    with open(\"control_KRR_notebook_example\",'r') as control_file:\n",
    "        for line in control_file:\n",
    "            if \"features_training \" in line:\n",
    "                file_features_training = line.split()[-1]\n",
    "            if \"dfpt_training \" in line:\n",
    "                file_dfpt_training = line.split()[-1]\n",
    "            if \"polar_mol_training \" in line:\n",
    "                file_molpol_training = line.split()[-1]\n",
    "                baselining = True\n",
    "            if \"features_test \" in line:\n",
    "                file_features_test = line.split()[-1]\n",
    "            if \"dfpt_test \" in line:\n",
    "                file_dfpt_test = line.split()[-1]\n",
    "            if \"polar_mol_test \" in line:\n",
    "                file_molpol_test = line.split()[-1]\n",
    "                baselining = True\n",
    "            #if \"length_training \" in line:\n",
    "            #    N_training = int(line.split()[-1])\n",
    "            if \"length_test \" in line:\n",
    "                N_test = int(line.split()[-1])\n",
    "            if \"grid_search \" in line:\n",
    "                if str(line.split()[-1])==\"yes\":\n",
    "                    grid_search = True\n",
    "                elif str(line.split()[-1])==\"no\":\n",
    "                    grid_search = False\n",
    "                else:\n",
    "                    print(\"What do you want in life?\\nMake a choice and come back when you are ready.\\nExiting (bad keyword for \\'grid_search\\').\")\n",
    "                    exit()\n",
    "            if \"plot \" in line:\n",
    "                if str(line.split()[-1])==\"yes\":\n",
    "                    plot_Raman=True\n",
    "                elif str(line.split()[-1])==\"no\":\n",
    "                    plot_Raman=False\n",
    "                else:\n",
    "                    print(\"What do you want in life?\\nMake a choice and come back when you are ready.\\nExiting (bad keyword for \\'plot\\').\")\n",
    "                    exit()\n",
    "    \n",
    "    # b) Read necessary files for the training set\n",
    "    \n",
    "    #print (\"You have chosen\", N_training, \"data points in the training set\")\n",
    "    \n",
    "\n",
    "    # Read descriptors\n",
    "    u_training = []\n",
    "    u_training = read_data(file_features_training,u_training)\n",
    "    if (len(u_training) < N_training):\n",
    "        print (\"The number of training points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "\n",
    "    # Shuffle data set. The following will produce a list of random indices of length the total size of the training set file\n",
    "    index_shuf = np.array(list(range(len(u_training))))\n",
    "    np.random.shuffle(index_shuf)\n",
    "    \n",
    "    # Use same indexes for all calculations, this way when increasing N_train, we always add new training points\n",
    "    index_shuf = np.genfromtxt(\"indices_default.dat\", usecols = 0, dtype=int,unpack=True)\n",
    "\n",
    "    # Print indices used for this run\n",
    "    #f_indices=open(\"indices_Ntrain\"+str(N_training)+'.dat','w')\n",
    "    #for a in index_shuf[0:N_training]: \n",
    "    #    f_indices.write( (\"{:d}\".format(a)) + \"\\n\" )\n",
    "    #f_indices.close()\n",
    "\n",
    "    #np.random.shuffle(index_shuf)\n",
    "\n",
    "    # First shuffle the data... The same shuffling has to be applied to all the training data, not just the descriptors\n",
    "    u_training = shuffle_data(u_training,index_shuf)\n",
    "    #...then select the first N_training values, as requested in the control file\n",
    "    u_training = select_data(u_training,N_training)\n",
    "    \n",
    "    # Read training DFPT polarizabilities\n",
    "    polar_dfpt_training = []\n",
    "    polar_dfpt_training = read_data(file_dfpt_training,polar_dfpt_training)\n",
    "    polar_dfpt_training = shuffle_data(polar_dfpt_training,index_shuf)\n",
    "    polar_dfpt_training = select_data(polar_dfpt_training,N_training)\n",
    "    mean_dfpt_training = np.mean(polar_dfpt_training,axis=0)\n",
    "    #print (\"Mean DFPT polarizability (training): \", mean_dfpt_training)\n",
    "    if (len(polar_dfpt_training) < N_training):\n",
    "        print (\"The number of training points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "\n",
    "    # Read sum of molecular polarizabilities (used for baselining, does not enter kernel)\n",
    "    if baselining:\n",
    "        molpol_training = []\n",
    "        molpol_training = read_data(file_molpol_training,molpol_training)\n",
    "        molpol_training = shuffle_data(molpol_training,index_shuf)\n",
    "        molpol_training = select_data(molpol_training,N_training)\n",
    "        if (len(molpol_training) < N_training):\n",
    "            print (\"The number of training points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "            exit()\n",
    "        mean_molpol_training = np.mean(molpol_training,axis=0)\n",
    "        print ('Average sum of molecular polarizabilities (training): ', mean_molpol_training)\n",
    "\n",
    "    \n",
    "    # c) Read necessary files for the test set\n",
    "    \n",
    "    #print (\"You chose\", N_test, \"data points to extrapolate.\")\n",
    "    \n",
    "    # Read descriptors \n",
    "    u_test = []\n",
    "    u_test = read_data(file_features_test,u_test)\n",
    "    if (len(u_test) < N_test):\n",
    "        print (\"The number of test points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "    # Select first N_test points\n",
    "    u_test = select_data(u_test,N_test)\n",
    "\n",
    "    \n",
    "    # Read the DFPT polarizabilities (only used to calculate the error we make with our predictive model, in principles we do not have access to them)\n",
    "    polar_dfpt_test = []\n",
    "    polar_dfpt_test = read_data(file_dfpt_test,polar_dfpt_test)\n",
    "    polar_dfpt_test = select_data(polar_dfpt_test,N_test)\n",
    "    mean_dfpt_test = np.mean(polar_dfpt_test,axis=0)\n",
    "    #print (\"Mean DFPT polarizability (test): \", mean_dfpt_test)\n",
    "    if (len(polar_dfpt_test) < N_test):\n",
    "        print (\"The number of test points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "\n",
    "    # Read sum of molecular polarizabilities (used for baselining)\n",
    "    if baselining:\n",
    "        molpol_test = []\n",
    "        molpol_test = read_data(file_molpol_test,molpol_test)\n",
    "        molpol_test = select_data(molpol_test,N_test)\n",
    "    \n",
    "\n",
    "    # II) Now the real work will start: start constructing the kernel, weights, etc. for a given Lambda and sigma\n",
    " \n",
    "    # Open files to store errors and predictions \n",
    "\n",
    "    folders = [\"errors\",\"predictions\",\"plots\"]\n",
    "    for d_ in folders:\n",
    "        if not os.path.exists(d_):\n",
    "            os.mkdir(d_)\n",
    "\n",
    "#     f_training=open(\"errors/error_training_\"+str(N_training)+'.dat','w')\n",
    "#     f_test=open(\"errors/error_Ntest\"+str(N_test)+\"_Ntrain\"+str(N_training)+'.dat','w')\n",
    "#     f_polar_training=open(\"predictions/predict_polar_training_\"+str(N_training)+'.dat','w')\n",
    "#     f_polar_test=open(\"predictions/predict_polar_test_Ntest\"+str(N_test)+\"_Ntrain\"+str(N_training)+'.dat','w')\n",
    "#     f_Raman_predict_training=open(\"predictions/polar_ML_training_\"+str(N_training)+'.dat','w')\n",
    "#     f_Raman_dfpt_training=open(\"predictions/polar_dfpt_training_\"+str(N_training)+'.dat','w')\n",
    "    name_raman_dfpt = \"polar_dfpt_test_\"+str(N_test)+\".dat\"\n",
    "#     name_raman_ML = \"polar_ML_test_Ntest\"+str(N_test)+\"_Ntrain\"+str(N_training)+\".dat\"\n",
    "    name_raman_ML = \"polar_ML_test.dat\"\n",
    "    f_Raman_predict_test=open(\"predictions/\" + name_raman_ML,'w')\n",
    "    f_Raman_dfpt_test=open(\"predictions/\" + name_raman_dfpt,'w')\n",
    "    \n",
    "    error_old = -1 # This will serve to determine the optimal hyperparameter\n",
    "    \n",
    "    repeat = True # Says whether to make another prediction with updated hyperparameters\n",
    "    first = True # First pass to the loop\n",
    "    while(repeat == True):\n",
    "        if(grid_search and first):\n",
    "            first = False\n",
    "        elif(grid_search and not first):\n",
    "            sigma,Lambda,repeat = vary_hyper(sigma,Lambda,sigma_min,sigma_max,Lambda_min,Lambda_max)\n",
    "        else: # Standard values\n",
    "            #sigma = 3e-3 # Good value for the paracetamol molecule with a few hundreds of training points \n",
    "            #sigma = 1e-2 \n",
    "            #Lambda = 1e-5 \n",
    "            repeat = False\n",
    "\n",
    "      # 1) Now predict the polarizability tensors of each training structure. The predicted value should almost be exact, but some deviation is allowed.\n",
    "      # The optimal lambda and sigma have to be determined against a validation test.\n",
    "      \n",
    "      # Construct the square-exponential kernel matrix, which has dimensions N_training*N_training\n",
    "      # k(i,j)=exp(-|u_i-u_j|^2/(2*sigma^2)) with u containing geometrical features\n",
    "      \n",
    "        #print (\"Constructing kernel for the training set...\")\n",
    "        \n",
    "        start = time.time() \n",
    "        t1 = time.time()\n",
    "        \n",
    "        eucl = scipy.spatial.distance.pdist(u_training[:,:],metric='sqeuclidean') # Dense matrix containing the norms \n",
    "        eucl = scipy.spatial.distance.squareform(eucl) # Put the previous matrix back in square form, with redundancies. It is of size N_training*N_training, and its elements are (u_ai-u_bi)**2, where u_ai is the ith feature of the ath configuration\n",
    "        \n",
    "        t2 = time.time()\n",
    "        #print('Took ',t2-t1, 'seconds')\n",
    "        \n",
    "        # Build the kernel\n",
    "        k_training = np.exp(-eucl/(2*sigma**2))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        #print (\"Took \", t2-t1, \"seconds\")\n",
    "        \n",
    "        # Calculate the weights that will be used to \"predict\" polarizabilities:\n",
    "        \n",
    "        #print (\"Inverting matrix...\\n\")\n",
    "        \n",
    "        L = Lambda*np.identity(N_training)\n",
    "        inv = np.linalg.inv(k_training+L)\n",
    "        \n",
    "        #print (\"Took \", time.time()-start, \"seconds\")\n",
    "        \n",
    "        #print (\"Calculating weights...\\n\")\n",
    "        \n",
    "        if (not baselining):\n",
    "            weights = np.dot(inv,(polar_dfpt_training - mean_dfpt_training))\n",
    "        else:\n",
    "            weights = np.dot(inv,(polar_dfpt_training - mean_dfpt_training) - (molpol_training - mean_molpol_training) )\n",
    "        \n",
    "        #print (\"Took \", time.time()-start, \"seconds\")\n",
    "        \n",
    "        #print (\"Predicting training set polarizabilities (although we have trained on the same points)\\n\")\n",
    "        #print (\"lambda = \", Lambda, \", sigma = \", sigma)\n",
    "        \n",
    "        # alpha_ML = alpha_mean_training + sum_1^N w_l k(u,u_l)\n",
    "        if (not baselining):\n",
    "            polar_training = np.dot(k_training,weights) + mean_dfpt_training # Add mean only if it was retrieved from the weights\n",
    "        else:\n",
    "            polar_training = np.dot(k_training,weights) + mean_dfpt_training + (molpol_training-mean_molpol_training)\n",
    "    \n",
    "        # Calculates error\n",
    "        #print(\"Training:\")\n",
    "#         print_error(sigma,polar_dfpt_training, polar_training,f_training,f_polar_training,f_Raman_predict_training,f_Raman_dfpt_training,N_training)\n",
    "    \n",
    "        # 2) Predict data for structures the model has not seen before (test set)\n",
    "        #print (\"Predict polarizabilities for the test set: be ready, it's starting!\")\n",
    "        \n",
    "        # Calculate the \"kernel\". Note that it is a priori not a square matrix here (we don't need to invert it afterwards), since it has dimensions N_test x N_training, with N_test the number of points to be predicted\n",
    "        \n",
    "        #print (\"Constructing \\\"kernel\\\"...\")\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        eucl = scipy.spatial.distance.cdist(u_test[:,:],u_training[:,:],metric='sqeuclidean') # Dense matrix containing the norms\n",
    "        # Build kernel\n",
    "        k_test = np.exp(-eucl/(2*sigma**2))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        #print('Took ',t2-t1, 'seconds')\n",
    "        \n",
    "        #print (\"Predicting values...\")\n",
    "        \n",
    "        if (not baselining):\n",
    "            polar_test = np.dot(k_test,weights) + mean_dfpt_training # Add mean only if it was retrieved from the weights\n",
    "        else:\n",
    "            polar_test = np.dot(k_test,weights) + mean_dfpt_training + (molpol_test - mean_molpol_training) \n",
    "        \n",
    "        # Print the error on the test set\n",
    "        #print(\"Test:\")\n",
    "#         mae = print_error(sigma,polar_dfpt_test, polar_test,f_test,f_polar_test,f_Raman_predict_test,f_Raman_dfpt_test,N_training)\n",
    "        mae = print_error(sigma,polar_dfpt_test, polar_test,f_Raman_predict_test,f_Raman_dfpt_test,N_training)\n",
    "    \n",
    "        # Find optimal hyperparameters\n",
    "        if(mae[0] < error_old or error_old == -1): # Check only xx at the moment\n",
    "            sigma_opt = sigma\n",
    "            Lambda_opt = Lambda\n",
    "            error_old = mae[0]\n",
    "    \n",
    "    if (grid_search):\n",
    "        print(\"Optimal hyperparameters (sigma,lambda) = ({:.2e},{:.2e})\".format(sigma_opt,Lambda_opt))\n",
    "\n",
    "    # Close all files\n",
    "#     f_training.close()\n",
    "#     f_test.close()\n",
    "#     f_polar_training.close()\n",
    "#     f_polar_test.close()\n",
    "#     f_Raman_predict_training.close()\n",
    "#     f_Raman_dfpt_training.close()\n",
    "    f_Raman_predict_test.close()\n",
    "    f_Raman_dfpt_test.close()\n",
    "    \n",
    "\n",
    "    #print(polar_test.shape)\n",
    "    #plt.figure(figsize=(12,6))\n",
    "    #plt.plot(polar_dfpt_test, polar_test, linestyle='',marker='o',markersize=9, color = 'r')\n",
    "    #plt.show()\n",
    "    \n",
    "    #plot_polar_series(polar_dfpt_test,polar_test,0,400,155,180)\n",
    "    \n",
    "        \n",
    "    # Calculate Raman spectrum if requested\n",
    "#     if (plot_Raman and not grid_search):\n",
    "#         print(\"\\nNow computing Raman spectra\\n\")\n",
    "#         compute_Raman(name_raman_dfpt,name_raman_ML)\n",
    "#         plot_raman('plots/','Raman_'+name_raman_dfpt,'Raman_'+name_raman_ML,0,4000,0.2)\n",
    "\n",
    "    intensity,intensity_krr=compute_Raman(name_raman_dfpt,name_raman_ML)\n",
    "    #plot_raman('plots/','Raman_'+name_raman_dfpt,'Raman_'+name_raman_ML,0,4000,0.3)\n",
    "    \n",
    "    return polar_dfpt_test,polar_test,intensity,intensity_krr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fac6510d9c48d2bb38f1caf6f14637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(label='time step', scale=LinearScale(max=1000.0, min=0.0)), Axis(label='$\\\\al…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5146e4408a43d28a15be5fc2d01dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(axes=[Axis(label='Wavenumber (cm$^{-1}$)', scale=LinearScale(max=4000.0, min=0.0)), Axis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Just initialize\n",
    "polar_dfpt_test,polar_test,intensity,intensity_krr = krr(2,0.5,0.01)\n",
    "plot_polar_series(polar_dfpt_test,polar_test,0,1000,155,180)\n",
    "plot_raman('plots/','Raman_polar_dfpt_test_2000.dat','Raman_polar_ML_test.dat',0,4000,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Accordion, IntSlider, FloatSlider, HTMLMath, Dropdown, Box, HBox, VBox, Layout\n",
    "from IPython.display import display\n",
    "N_training_widget = widgets.IntSlider(min=1, max=700, step=1, value=10, description = \"$N_{training}$\", continuous_update=False)\n",
    "#sigma_widget = widgets.FloatSlider(min=0.01, max=10, step=0.1, value=0.2, description = \"sigma\", continuous_update=False)\n",
    "sigma_widget = widgets.FloatLogSlider(value=0.1,base=10,min=-1, max=2, step=0.2, description='sigma', continuous_update=False)\n",
    "Lambda_widget = widgets.FloatLogSlider(value=0.001,base=10,min=-8, max=1, step=0.5, description='lambda', continuous_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_change0(change):\n",
    "    polar_dfpt_test,polar_test,intensity_dfpt,intensity_krr = krr(\n",
    "        N_training=N_training_widget.value,\n",
    "        sigma=sigma_widget.value,Lambda=Lambda_widget.value)\n",
    "    \n",
    "    number = [i for i in range(len(polar_dfpt_test))]\n",
    "\n",
    "    seg_pl0.x=number\n",
    "    seg_pl0.y=polar_dfpt_test[:,0]\n",
    "    seg_pl1.x=number\n",
    "    seg_pl1.y=polar_test[:,0]\n",
    "    \n",
    "    raman0.y=intensity_dfpt\n",
    "    raman1.y=intensity_krr\n",
    "\n",
    "#n_widget.observe(on_graph_params_change_nopath, names='value', type='change')\n",
    "#n_threshold_widget.observe(on_graph_params_change_nopath, names='value', type='change')\n",
    "N_training_widget.observe(on_change0, names='value', type='change')\n",
    "sigma_widget.observe(on_change0, names='value', type='change')\n",
    "Lambda_widget.observe(on_change0, names='value', type='change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1b5175230346d2aafbb9396dba1bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(VBox(children=(IntSlider(value=10, continuous_update=False, description='$N_{training}$', max=70…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cf26545e0443008fdd161f5f7f91c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Figure(axes=[Axis(label='time step', scale=LinearScale(max=1000.0, min=0.0), side='bottom'), Axi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Box([\n",
    "        VBox([N_training_widget,sigma_widget,Lambda_widget], layout=Layout(width='350px'))],\n",
    "        layout=Layout(width='100%', flex_flow='row wrap', display='flex')))\n",
    "\n",
    "plot_time_series.layout.min_width = '500px'\n",
    "plot_time_series.layout.max_width = '500px'\n",
    "plot_time_series.layout.min_height = '400px'\n",
    "plot_time_series.layout.max_height = '400px'\n",
    "display(Box(children=[plot_time_series,plot_raman_spec], layout=Layout(justify_content='center')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_time_series = pl.figure()\n",
    "#krr(10,1)\n",
    "#pl.current_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seg_pl0.y=[170,175,180]\n",
    "#seg_pl0.x=[0,1,2]\n",
    "#print(seg_pl0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interact_manual(krr,N_training=widgets.IntSlider(min=1, max=1000, step=1, value=10),sigma=widgets.FloatSlider(min=0.1, max=100, step=0.5, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt_test.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interact(krr,N_training=widgets.IntSlider(min=1, max=300, step=1, value=2),sigma=widgets.FloatSlider(min=0.1, max=100, step=0.5, value=1),continuous_update=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_raman_dfpt = \"polar_dfpt_test_6000.dat\"\n",
    "# name_raman_ML = \"polar_ML_test_Ntest6000_Ntrain2.dat\"\n",
    "# print(\"\\nNow computing Raman spectra...\\n\")\n",
    "# compute_Raman(name_raman_dfpt,name_raman_ML)\n",
    "# plot_raman('plots/','Raman_'+name_raman_dfpt,'Raman_'+name_raman_ML,0,4000,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Understanding kernel ridge regression: Common behaviors from simple functions to density functionals, https://doi.org/10.1002/qua.24939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
