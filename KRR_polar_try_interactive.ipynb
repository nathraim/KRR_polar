{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.special import erf\n",
    "import scipy.spatial\n",
    "import os,argparse\n",
    "import random\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "def file_len(fname):\n",
    "    '''\n",
    "    This function gives the number of lines of a given text file\n",
    "    '''\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "def read_txt(name,array):\n",
    "    '''\n",
    "    The following function reads a text file containing alternatively one line of comment and one line of data, and returns a numpy array\n",
    "    '''\n",
    "    name = os.path.join('data', name)\n",
    "    N_points = int(file_len(name)/2)\n",
    "    #print ('There are', N_points, 'points in the file', name)\n",
    "    data = open(name,'r')\n",
    "    array = []\n",
    "    counter = 0\n",
    "    while (counter < N_points):\n",
    "        line = data.readline() # Skip line of comment\n",
    "        if not line:\n",
    "            break\n",
    "        line = data.readline() # Line containing the information wanted\n",
    "        array.append(list(map(float, line.split()[0:])))\n",
    "        counter = counter+1\n",
    "    data.close()\n",
    "    array = np.array(array)\n",
    "    return array\n",
    "\n",
    "# The following function reads a numpy file (.npy) containing the quantity of interest stored directly as an array.\n",
    "\n",
    "def read_npy(name,array):\n",
    "    '''\n",
    "    The following function reads a numpy array.\n",
    "    '''\n",
    "    name = os.path.join('data', name)\n",
    "#   Load the saved numpy array\n",
    "    array_reloaded = np.load(name)\n",
    "#   Determine length of the array (corresponds to the number of training (or test) points)\n",
    "    N_points = len(array_reloaded)\n",
    "    #print ('There are', N_points, 'points in the file', name)\n",
    "    return array_reloaded\n",
    "\n",
    "def read_data(name,array):\n",
    "    '''\n",
    "    Reads data either as text or numpy format\n",
    "    '''\n",
    "    extension = os.path.splitext(name)[1]\n",
    "    if (extension == '.dat'):\n",
    "        array = read_txt(name,array)\n",
    "    elif (extension == '.npy'):\n",
    "        array = read_npy(name,array)\n",
    "    else:\n",
    "        print ('Extension for %s unrecognized !'.format(name))\n",
    "        exit()\n",
    "    return array\n",
    "\n",
    "\n",
    "def shuffle_data(array,index_shuf):\n",
    "    '''\n",
    "    Shuffle an array according to the list of indexes given in input\n",
    "    '''\n",
    "    array_shuf = []\n",
    "    for i in index_shuf:\n",
    "        array_shuf.append(array[i])\n",
    "    return array_shuf\n",
    "\n",
    "def select_data(array,N):\n",
    "    '''\n",
    "    Selects only the first N values of the input array\n",
    "    '''\n",
    "    array = array[0:N]\n",
    "    array = np.array(array)\n",
    "    return array\n",
    "\n",
    "def print_error(sigma,polar_dfpt, polar_ML,f_,f_polar,f_Raman_predict,f_Raman_dfpt,N_training):\n",
    "    '''\n",
    "    Print error on screen and write it to file\n",
    "    '''\n",
    "    mae = np.mean(abs(polar_dfpt - polar_ML),axis=0) # MAE\n",
    "    max_error = np.amax(abs(polar_dfpt - polar_ML),axis=0)\n",
    "    min_error = np.amin(abs(polar_dfpt - polar_ML),axis=0)\n",
    "    rmse = np.sqrt(np.mean((polar_dfpt - polar_ML)**2,axis=0)) # RMSE\n",
    "    std = np.std(polar_dfpt,axis=0) # STD\n",
    "    rmse_normal = 100*rmse/std\n",
    "#     print (\"Error                     |      xx          yy          zz          xy          xz          yz\")\n",
    "#     print (\"--------------------------|---------------------------------------------------------------------\")\n",
    "#     print (\"MAE                       |\", end=' ')\n",
    "#     for a in mae: print (\"{0:.4e}\".format(a),end=' ')\n",
    "#     print ('')\n",
    "#     print (\"Max                       |\",end=' ')\n",
    "#     for a in max_error: print (\"{0:.4e}\".format(a),end=' ')\n",
    "#     print ('')\n",
    "#     print (\"Min                       |\",end=' ')\n",
    "#     for a in min_error: print (\"{0:.4e}\".format(a),end=' ')\n",
    "#     print ('')\n",
    "#     print (\"RMSE/STD                  |\", end= ' ')\n",
    "#     for a in rmse_normal: print (\"{0:.4e}\".format(a), end='  ')\n",
    "#     print ('')\n",
    "\n",
    "    # Write to file the error versus sigma and Lambda\n",
    "    f_.write( (\"{0:.2e}\".format(sigma)) + \" \" + \"{0:.2e}\".format(Lambda) + \" \" +(\"{:d}\".format(N_training)) + \" \" )\n",
    "    for a in mae: f_.write( (\"{0:.5e}\".format(a)) + \" \" )\n",
    "    for a in rmse_normal: f_.write( (\"{0:.5e}\".format(a)) + \" \" )\n",
    "    f_.write(\"\\n\")\n",
    "\n",
    "    f_polar.write(\"# num | polar_DFPTxx | predictionxx | polar_DFPTyy | predictionyy | ... \")\n",
    "    for a in rmse_normal: f_polar.write( (\"{0:.5e}\".format(a)) + \" \" )\n",
    "    f_polar.write(\"\\n\")\n",
    "    for i in range(len(polar_dfpt)):\n",
    "        f_polar.write(str(i)+\" \")\n",
    "        for comp in range(6):\n",
    "            f_polar.write(str(polar_dfpt[i,comp]) + \" \" + str(polar_ML[i,comp]) + \" \")\n",
    "        f_polar.write(\"\\n\")\n",
    "\n",
    "    for i in range(len(polar_dfpt)):\n",
    "        f_Raman_predict.write(str(polar_ML[i,0]) + \" \" + str(polar_ML[i,1]) + \" \" + str(polar_ML[i,2]) + \" \" + str(polar_ML[i,3]) + \" \" + str(polar_ML[i,4]) + \" \" + str(polar_ML[i,5]) + '\\n' )\n",
    "\n",
    "    # Write the corresponding true spectrum (i.e. DFPT on training set) \n",
    "    for i in range(len(polar_dfpt)):\n",
    "        f_Raman_dfpt.write(str(polar_dfpt[i,0]) + \" \" + str(polar_dfpt[i,1]) + \" \" + str(polar_dfpt[i,2]) + \" \"  + str(polar_dfpt[i,3]) + \" \" + str(polar_dfpt[i,4]) + \" \" + str(polar_dfpt[i,5]) + '\\n' )\n",
    "\n",
    "    return mae\n",
    "\n",
    "\n",
    "def compute_Raman(name_raman_dfpt,name_raman_ML):\n",
    "    '''\n",
    "    Computes polarizability autocorrelation function and Raman spectrum\n",
    "    '''\n",
    "    name_list = \"list.dat\" # Input file for autocorrelation script\n",
    "    os.chdir(\"predictions\")\n",
    "    \n",
    "    with open(name_list,'w') as f_:\n",
    "        f_.write(name_raman_dfpt + ' ') # The space is needed so that the autocorrelation script parses the name correctly\n",
    "    # Calculate autocorrelation and Raman spectrum of dfpt\n",
    "    os.system(\"python3 autocorr.py control.autocorr.in\")\n",
    "    \n",
    "    with open(name_list,'w') as f_:\n",
    "        f_.write(name_raman_ML + ' ')\n",
    "    # Calculate autocorrelation and Raman spectrum of prediction\n",
    "    os.system(\"python3 autocorr.py control.autocorr.in\")\n",
    "    \n",
    "    # Move Raman data to the \"plots\" directory\n",
    "    os.chdir(\"..\")\n",
    "    os.rename('predictions/Raman_'+name_raman_dfpt, 'plots/Raman_'+name_raman_dfpt)\n",
    "    os.rename('predictions/Raman_'+name_raman_ML, 'plots/Raman_'+name_raman_ML)\n",
    "    \n",
    "def plot_raman(file_path, file_name, file_name2,xmin,xmax,ymax):\n",
    "\n",
    "##################### Read Data #####################\n",
    "\n",
    "    results_file_dir = os.path.join(file_path, file_name)\n",
    "    results_file = open(results_file_dir,'r')\n",
    "    my_file = 'plot_raman_krrvsdfpt.png'    \n",
    "        \n",
    "    freqs = []\n",
    "    intensity = []\n",
    "      \n",
    "    for line in results_file:\n",
    "        \n",
    "        freqs.append(line.split()[0])\n",
    "        intensity.append(line.split()[1])\n",
    "    \n",
    "    results_file.close()\n",
    "    \n",
    "    results_file_dir = os.path.join(file_path, file_name2)\n",
    "    results_file = open(results_file_dir,'r')\n",
    "    \n",
    "    freqs_krr = []\n",
    "    intensity_krr = []\n",
    "    \n",
    "    for line in results_file:\n",
    "        \n",
    "        freqs_krr.append(line.split()[0])\n",
    "        intensity_krr.append(line.split()[1])\n",
    "        \n",
    "    results_file.close()\n",
    "    #print(freqs)\n",
    "            \n",
    "      ##################### Plotting #####################\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    x = [float(i) for i in freqs]\n",
    "    y = [float(i) for i in intensity]        \n",
    "    plt.plot(x, y, '-', lw=2.5, color = 'black')\n",
    "    x = [float(i) for i in freqs_krr]\n",
    "    y = [float(i) for i in intensity_krr] \n",
    "    plt.plot(x, y, '-', lw=2.5, color = 'orange')\n",
    "\n",
    "    #plt.plot(np.array(freqs), np.array(intensity), '-', lw=2.5, color = 'black')\n",
    "    #plt.plot(freqs_krr, intensity_krr, '-', lw=2.5, color = 'orange')\n",
    "    plt.plot(markersize=5,markeredgewidth=5)\n",
    "            \n",
    "    components = ['DFPT','GPR']\n",
    "\n",
    "    plt.legend(components, bbox_to_anchor=(0.55, 0.9), loc='upper left', fontsize=24, frameon=False)\n",
    "    \n",
    "    plt.xticks(fontsize=24)\n",
    "    plt.yticks(fontsize=24)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    plt.ylim(0,ymax)\n",
    "    plt.xlabel('Wavenumber (cm$^{-1}$)', fontsize=28)\n",
    "    plt.ylabel('$I(\\\\omega)$', fontsize=28)\n",
    "    plt.gca().axes.get_yaxis().set_ticks([]) # Removes ticks for y axis     \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_polar_series(polar_dfpt,polar_krr,xmin,xmax,ymin,ymax):\n",
    "    import matplotlib.font_manager as font_manager\n",
    "            \n",
    "      ##################### Plotting #####################\n",
    "        \n",
    "    number = [i for i in range(len(polar_dfpt))]\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    plt.plot(number, polar_dfpt[:,0], '-', color = 'black',lw=2)\n",
    "    plt.plot(number, polar_krr[:,0], '-', color = 'orange',lw=2)\n",
    "\n",
    "    #plt.plot(number, zz_actual, '-', color = 'c')\n",
    "    #plt.plot(number, zz_predict, '-', color = 'y')\n",
    "\n",
    "    components = ['DFPT','GPR']\n",
    "\n",
    "    plt.legend(components, bbox_to_anchor=(0.10, 0.97), loc='upper left', fontsize=24,frameon=False,prop=font_manager.FontProperties(weight='normal',style='normal', size=26))\n",
    "\n",
    "    plt.xlim(xmin,xmax)\n",
    "    plt.ylim(ymin,ymax)\n",
    "    plt.xticks(fontsize=28)\n",
    "    plt.yticks(fontsize=28)\n",
    "    plt.xlabel('time step', fontsize=32)\n",
    "    plt.ylabel('$\\\\alpha_{xx}$', fontsize=32)\n",
    "    #plt.savefig('plot_polar_series_xx', bbox_inches ='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Define maximum and minimum values for hyperparameters in case a grid search is desired\n",
    "Lambda_min = 1.e-8\n",
    "Lambda_max = 1\n",
    "sigma_min = 1.e-2\n",
    "sigma_max = 100\n",
    "#sigma = sigma_min # Current sigma\n",
    "Lambda = Lambda_min # current Lambda\n",
    "#sigma_opt = sigma # Optimal sigma\n",
    "Lambda_opt = Lambda # Optimal lambda\n",
    "\n",
    "def vary_hyper(sigma,Lambda,sigma_min,sigma_max,Lambda_min,Lambda_max):\n",
    "    '''\n",
    "    Vary hyperparameters when a grid search is requested\n",
    "    '''\n",
    "    if (Lambda<Lambda_max):\n",
    "        Lambda=Lambda*10**1\n",
    "        repeat = True\n",
    "    elif (sigma<sigma_max):\n",
    "        #sigma = sigma*10**0.25\n",
    "        sigma = sigma*10**0.5\n",
    "        Lambda = Lambda_min\n",
    "        repeat = True\n",
    "    else:\n",
    "        repeat = False\n",
    "    return sigma,Lambda,repeat\n",
    "  \n",
    "\n",
    "def krr(N_training,sigma):\n",
    "#if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description = ''' This program will interpolate polarizability tensors based on their geometrical features (atomic coordinates, atomic density, etc.) \n",
    "# Al  l specifications are given in the \\'control_KRR\\' file.\n",
    "#     It requires at least 2 files for the training set:\n",
    "#      - 1 file containing the geometrical information of your system for each structure (i.e., atomic coordinates, atomic densities, etc.)\n",
    "#      - 1 file containing the polarizabilities (or dipoles, etc.) corresponding to the aforementioned structures \n",
    "#     and 1 file for the test set:\n",
    "#      - 1 file containing the geometrical information of your system for each structure, different from the training set\n",
    "#     After training, the routine will predict the polarizability tensors of the new (test) set of structures.\n",
    "#     The script accepts data as text (\\'.dat\\' files) or numpy format (\\'.npy\\' files). In the case of text data, it is expected that there is 1 alternatively 1 line of comment, and 1 line of data, with the elements to be predicted being separated by spaces\n",
    "#     ''',formatter_class=argparse.RawTextHelpFormatter)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "\n",
    "    # I) Preparation\n",
    "    \n",
    "    baselining = False\n",
    "    #N_training = sys.argv[1]\n",
    "\n",
    "    # a) Read control file\n",
    "    with open(\"control_KRR_notebook_example\",'r') as control_file:\n",
    "        for line in control_file:\n",
    "            if \"features_training \" in line:\n",
    "                file_features_training = line.split()[-1]\n",
    "            if \"dfpt_training \" in line:\n",
    "                file_dfpt_training = line.split()[-1]\n",
    "            if \"polar_mol_training \" in line:\n",
    "                file_molpol_training = line.split()[-1]\n",
    "                baselining = True\n",
    "            if \"features_test \" in line:\n",
    "                file_features_test = line.split()[-1]\n",
    "            if \"dfpt_test \" in line:\n",
    "                file_dfpt_test = line.split()[-1]\n",
    "            if \"polar_mol_test \" in line:\n",
    "                file_molpol_test = line.split()[-1]\n",
    "                baselining = True\n",
    "            #if \"length_training \" in line:\n",
    "            #    N_training = int(line.split()[-1])\n",
    "            if \"length_test \" in line:\n",
    "                N_test = int(line.split()[-1])\n",
    "            if \"grid_search \" in line:\n",
    "                if str(line.split()[-1])==\"yes\":\n",
    "                    grid_search = True\n",
    "                elif str(line.split()[-1])==\"no\":\n",
    "                    grid_search = False\n",
    "                else:\n",
    "                    print(\"What do you want in life?\\nMake a choice and come back when you are ready.\\nExiting (bad keyword for \\'grid_search\\').\")\n",
    "                    exit()\n",
    "            if \"plot \" in line:\n",
    "                if str(line.split()[-1])==\"yes\":\n",
    "                    plot_Raman=True\n",
    "                elif str(line.split()[-1])==\"no\":\n",
    "                    plot_Raman=False\n",
    "                else:\n",
    "                    print(\"What do you want in life?\\nMake a choice and come back when you are ready.\\nExiting (bad keyword for \\'plot\\').\")\n",
    "                    exit()\n",
    "    \n",
    "    # b) Read necessary files for the training set\n",
    "    \n",
    "    #print (\"You have chosen\", N_training, \"data points in the training set\")\n",
    "    \n",
    "\n",
    "    # Read descriptors\n",
    "    u_training = []\n",
    "    u_training = read_data(file_features_training,u_training)\n",
    "    if (len(u_training) < N_training):\n",
    "        print (\"The number of training points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "\n",
    "    # Shuffle data set. The following will produce a list of random indices of length the total size of the training set file\n",
    "    index_shuf = np.array(list(range(len(u_training))))\n",
    "    np.random.shuffle(index_shuf)\n",
    "    \n",
    "    # Use same indexes for all calculations, this way when increasing N_train, we always add new training points\n",
    "    #index_shuf = np.genfromtxt(\"index2.dat\", usecols = 0, dtype=int,unpack=True)\n",
    "\n",
    "    # Print indices used for this run\n",
    "    #f_indices=open(\"indices_Ntrain\"+str(N_training)+'.dat','w')\n",
    "    #for a in index_shuf[0:N_training]: \n",
    "    #    f_indices.write( (\"{:d}\".format(a)) + \"\\n\" )\n",
    "    #f_indices.close()\n",
    "\n",
    "    #np.random.shuffle(index_shuf)\n",
    "\n",
    "    # First shuffle the data... The same shuffling has to be applied to all the training data, not just the descriptors\n",
    "    u_training = shuffle_data(u_training,index_shuf)\n",
    "    #...then select the first N_training values, as requested in the control file\n",
    "    u_training = select_data(u_training,N_training)\n",
    "    \n",
    "    # Read training DFPT polarizabilities\n",
    "    polar_dfpt_training = []\n",
    "    polar_dfpt_training = read_data(file_dfpt_training,polar_dfpt_training)\n",
    "    polar_dfpt_training = shuffle_data(polar_dfpt_training,index_shuf)\n",
    "    polar_dfpt_training = select_data(polar_dfpt_training,N_training)\n",
    "    mean_dfpt_training = np.mean(polar_dfpt_training,axis=0)\n",
    "    #print (\"Mean DFPT polarizability (training): \", mean_dfpt_training)\n",
    "    if (len(polar_dfpt_training) < N_training):\n",
    "        print (\"The number of training points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "\n",
    "    # Read sum of molecular polarizabilities (used for baselining, does not enter kernel)\n",
    "    if baselining:\n",
    "        molpol_training = []\n",
    "        molpol_training = read_data(file_molpol_training,molpol_training)\n",
    "        molpol_training = shuffle_data(molpol_training,index_shuf)\n",
    "        molpol_training = select_data(molpol_training,N_training)\n",
    "        if (len(molpol_training) < N_training):\n",
    "            print (\"The number of training points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "            exit()\n",
    "        mean_molpol_training = np.mean(molpol_training,axis=0)\n",
    "        print ('Average sum of molecular polarizabilities (training): ', mean_molpol_training)\n",
    "\n",
    "    \n",
    "    # c) Read necessary files for the test set\n",
    "    \n",
    "    #print (\"You chose\", N_test, \"data points to extrapolate.\")\n",
    "    \n",
    "    # Read descriptors \n",
    "    u_test = []\n",
    "    u_test = read_data(file_features_test,u_test)\n",
    "    if (len(u_test) < N_test):\n",
    "        print (\"The number of test points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "    # Select first N_test points\n",
    "    u_test = select_data(u_test,N_test)\n",
    "\n",
    "    \n",
    "    # Read the DFPT polarizabilities (only used to calculate the error we make with our predictive model, in principles we do not have access to them)\n",
    "    polar_dfpt_test = []\n",
    "    polar_dfpt_test = read_data(file_dfpt_test,polar_dfpt_test)\n",
    "    polar_dfpt_test = select_data(polar_dfpt_test,N_test)\n",
    "    mean_dfpt_test = np.mean(polar_dfpt_test,axis=0)\n",
    "    #print (\"Mean DFPT polarizability (test): \", mean_dfpt_test)\n",
    "    if (len(polar_dfpt_test) < N_test):\n",
    "        print (\"The number of test points you have asked for is larger than the number of data points available \\n Exiting\")\n",
    "        exit()\n",
    "\n",
    "    # Read sum of molecular polarizabilities (used for baselining)\n",
    "    if baselining:\n",
    "        molpol_test = []\n",
    "        molpol_test = read_data(file_molpol_test,molpol_test)\n",
    "        molpol_test = select_data(molpol_test,N_test)\n",
    "    \n",
    "\n",
    "    # II) Now the real work will start: start constructing the kernel, weights, etc. for a given Lambda and sigma\n",
    " \n",
    "    # Open files to store errors and predictions \n",
    "\n",
    "    folders = [\"errors\",\"predictions\",\"plots\"]\n",
    "    for d_ in folders:\n",
    "        if not os.path.exists(d_):\n",
    "            os.mkdir(d_)\n",
    "\n",
    "    f_training=open(\"errors/error_training_\"+str(N_training)+'.dat','w')\n",
    "    f_test=open(\"errors/error_Ntest\"+str(N_test)+\"_Ntrain\"+str(N_training)+'.dat','w')\n",
    "    f_polar_training=open(\"predictions/predict_polar_training_\"+str(N_training)+'.dat','w')\n",
    "    f_polar_test=open(\"predictions/predict_polar_test_Ntest\"+str(N_test)+\"_Ntrain\"+str(N_training)+'.dat','w')\n",
    "    f_Raman_predict_training=open(\"predictions/polar_ML_training_\"+str(N_training)+'.dat','w')\n",
    "    f_Raman_dfpt_training=open(\"predictions/polar_dfpt_training_\"+str(N_training)+'.dat','w')\n",
    "    name_raman_dfpt = \"polar_dfpt_test_\"+str(N_test)+\".dat\"\n",
    "    name_raman_ML = \"polar_ML_test_Ntest\"+str(N_test)+\"_Ntrain\"+str(N_training)+\".dat\"\n",
    "    f_Raman_predict_test=open(\"predictions/\" + name_raman_ML,'w')\n",
    "    f_Raman_dfpt_test=open(\"predictions/\" + name_raman_dfpt,'w')\n",
    "    \n",
    "    error_old = -1 # This will serve to determine the optimal hyperparameter\n",
    "    \n",
    "    repeat = True # Says whether to make another prediction with updated hyperparameters\n",
    "    first = True # First pass to the loop\n",
    "    while(repeat == True):\n",
    "        if(grid_search and first):\n",
    "            first = False\n",
    "        elif(grid_search and not first):\n",
    "            sigma,Lambda,repeat = vary_hyper(sigma,Lambda,sigma_min,sigma_max,Lambda_min,Lambda_max)\n",
    "        else: # Standard values\n",
    "            #sigma = 3e-3 # Good value for the paracetamol molecule with a few hundreds of training points \n",
    "            #sigma = 1e-2 \n",
    "            Lambda = 1e-5 \n",
    "            repeat = False\n",
    "\n",
    "      # 1) Now predict the polarizability tensors of each training structure. The predicted value should almost be exact, but some deviation is allowed.\n",
    "      # The optimal lambda and sigma have to be determined against a validation test.\n",
    "      \n",
    "      # Construct the square-exponential kernel matrix, which has dimensions N_training*N_training\n",
    "      # k(i,j)=exp(-|u_i-u_j|^2/(2*sigma^2)) with u containing geometrical features\n",
    "      \n",
    "        #print (\"Constructing kernel for the training set...\")\n",
    "        \n",
    "        start = time.time() \n",
    "        t1 = time.time()\n",
    "        \n",
    "        eucl = scipy.spatial.distance.pdist(u_training[:,:],metric='sqeuclidean') # Dense matrix containing the norms \n",
    "        eucl = scipy.spatial.distance.squareform(eucl) # Put the previous matrix back in square form, with redundancies. It is of size N_training*N_training, and its elements are (u_ai-u_bi)**2, where u_ai is the ith feature of the ath configuration\n",
    "        \n",
    "        t2 = time.time()\n",
    "        #print('Took ',t2-t1, 'seconds')\n",
    "        \n",
    "        # Build the kernel\n",
    "        k_training = np.exp(-eucl/(2*sigma**2))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        #print (\"Took \", t2-t1, \"seconds\")\n",
    "        \n",
    "        # Calculate the weights that will be used to \"predict\" polarizabilities:\n",
    "        \n",
    "        #print (\"Inverting matrix...\\n\")\n",
    "        \n",
    "        L = Lambda*np.identity(N_training)\n",
    "        inv = np.linalg.inv(k_training+L)\n",
    "        \n",
    "        #print (\"Took \", time.time()-start, \"seconds\")\n",
    "        \n",
    "        #print (\"Calculating weights...\\n\")\n",
    "        \n",
    "        if (not baselining):\n",
    "            weights = np.dot(inv,(polar_dfpt_training - mean_dfpt_training))\n",
    "        else:\n",
    "            weights = np.dot(inv,(polar_dfpt_training - mean_dfpt_training) - (molpol_training - mean_molpol_training) )\n",
    "        \n",
    "        #print (\"Took \", time.time()-start, \"seconds\")\n",
    "        \n",
    "        #print (\"Predicting training set polarizabilities (although we have trained on the same points)\\n\")\n",
    "        print (\"lambda = \", Lambda, \", sigma = \", sigma)\n",
    "        \n",
    "        # alpha_ML = alpha_mean_training + sum_1^N w_l k(u,u_l)\n",
    "        if (not baselining):\n",
    "            polar_training = np.dot(k_training,weights) + mean_dfpt_training # Add mean only if it was retrieved from the weights\n",
    "        else:\n",
    "            polar_training = np.dot(k_training,weights) + mean_dfpt_training + (molpol_training-mean_molpol_training)\n",
    "    \n",
    "        # Calculates error\n",
    "        #print(\"Training:\")\n",
    "        print_error(sigma,polar_dfpt_training, polar_training,f_training,f_polar_training,f_Raman_predict_training,f_Raman_dfpt_training,N_training)\n",
    "    \n",
    "        # 2) Predict data for structures the model has not seen before (test set)\n",
    "        #print (\"Predict polarizabilities for the test set: be ready, it's starting!\")\n",
    "        \n",
    "        # Calculate the \"kernel\". Note that it is a priori not a square matrix here (we don't need to invert it afterwards), since it has dimensions N_test x N_training, with N_test the number of points to be predicted\n",
    "        \n",
    "        #print (\"Constructing \\\"kernel\\\"...\")\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        eucl = scipy.spatial.distance.cdist(u_test[:,:],u_training[:,:],metric='sqeuclidean') # Dense matrix containing the norms\n",
    "        # Build kernel\n",
    "        k_test = np.exp(-eucl/(2*sigma**2))\n",
    "        \n",
    "        t2 = time.time()\n",
    "        #print('Took ',t2-t1, 'seconds')\n",
    "        \n",
    "        #print (\"Predicting values...\")\n",
    "        \n",
    "        if (not baselining):\n",
    "            polar_test = np.dot(k_test,weights) + mean_dfpt_training # Add mean only if it was retrieved from the weights\n",
    "        else:\n",
    "            polar_test = np.dot(k_test,weights) + mean_dfpt_training + (molpol_test - mean_molpol_training) \n",
    "        \n",
    "        # Print the error on the test set\n",
    "        #print(\"Test:\")\n",
    "        mae = print_error(sigma,polar_dfpt_test, polar_test,f_test,f_polar_test,f_Raman_predict_test,f_Raman_dfpt_test,N_training)\n",
    "    \n",
    "        # Find optimal hyperparameters\n",
    "        if(mae[0] < error_old or error_old == -1): # Check only xx at the moment\n",
    "            sigma_opt = sigma\n",
    "            Lambda_opt = Lambda\n",
    "            error_old = mae[0]\n",
    "    \n",
    "    if (grid_search):\n",
    "        print(\"Optimal hyperparameters (sigma,lambda) = ({:.2e},{:.2e})\".format(sigma_opt,Lambda_opt))\n",
    "\n",
    "    # Close all files\n",
    "    f_training.close()\n",
    "    f_test.close()\n",
    "    f_polar_training.close()\n",
    "    f_polar_test.close()\n",
    "    f_Raman_predict_training.close()\n",
    "    f_Raman_dfpt_training.close()\n",
    "    f_Raman_predict_test.close()\n",
    "    f_Raman_dfpt_test.close()\n",
    "    \n",
    "\n",
    "    #print(polar_test.shape)\n",
    "    #plt.figure(figsize=(12,6))\n",
    "    #plt.plot(polar_dfpt_test, polar_test, linestyle='',marker='o',markersize=9, color = 'r')\n",
    "    #plt.show()\n",
    "    \n",
    "    plot_polar_series(polar_dfpt_test,polar_test,0,200,155,185)\n",
    "        \n",
    "    # Calculate Raman spectrum if requested\n",
    "#     if (plot_Raman and not grid_search):\n",
    "#         print(\"\\nNow computing Raman spectra\\n\")\n",
    "#         compute_Raman(name_raman_dfpt,name_raman_ML)\n",
    "#         plot_raman('plots/','Raman_'+name_raman_dfpt,'Raman_'+name_raman_ML,0,4000,0.2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57df431750ed46f3ba33760d95ffc30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='N_training', max=1000, min=1), FloatSlider(value=1.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.krr(N_training, sigma)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(krr,N_training=widgets.IntSlider(min=1, max=1000, step=1, value=10),sigma=widgets.FloatSlider(min=0.1, max=100, step=0.5, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interact(krr,N_training=widgets.IntSlider(min=1, max=300, step=1, value=2),sigma=widgets.FloatSlider(min=0.1, max=100, step=0.5, value=1),continuous_update=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_raman_dfpt = \"polar_dfpt_test_6000.dat\"\n",
    "# name_raman_ML = \"polar_ML_test_Ntest6000_Ntrain2.dat\"\n",
    "# print(\"\\nNow computing Raman spectra...\\n\")\n",
    "# compute_Raman(name_raman_dfpt,name_raman_ML)\n",
    "# plot_raman('plots/','Raman_'+name_raman_dfpt,'Raman_'+name_raman_ML,0,4000,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
